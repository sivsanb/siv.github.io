<!DOCTYPE html>
<html>
<body>
<h1>Welcome to Siv World</h1>
<p>
  About
Sr. Data Engineer | Google Cloud Platform | Azure | Multi Cloud Experience | Big Data | Apache Spark | Hadoop | Python | SQL Scripting | ETL | Oracle | Teradata | R | Pyspark

Experienced Sr.Data Engineer with 16+ years of experience in IT industry including various Cloud platforms and Big data environment.

- Experience includes Requirement analysis, Functional design, Technical high-level design and detailed level design, coding, testing and production implementation, warranty support.
- Extensive hands-on experience in developing and architecting Google Cloud Platform (GCP) projects involving services like BigQuery, Dataflow, Pub/Sub, Dataproc, App Engine, Cloud shell, gcloud and gsutil functions, bq command line utilities, Cloud Storage, Cloud Monitoring & Logging
- Experience in designing and creating data pipelines using various Cloud services and Big Data technologies.
- Multi Cloud Experience (Azure & GCP)
- Experience in Architecting Enterprise Data Lake including but not limited to Data Governance, Data Modeling (Data Vault, Dimensional, and Normal Form) and database design (RDBMS, NoSQL, and Hadoop), Data Classification, Data Integration, Enterprise Data and Metadata Management, Data Warehousing, Business Intelligence, Data Analysis and Data Security.
- Experience of working Azure and Google Cloud for running highly scalable and fast big data workloads using Apache Spark.
- Experience in Migration of Enterprise Data warehouses from Oracle/Teradata to GCP Native Big query Spark, Hive, Cloud Dataproc, Cloud Dataflow, Apache Beam/ composer, Cloud Big Query, Cloud Pub Sub, Cloud storage Cloud Functions ,Looker & GitHub.
- Experience managing Google Cloud Storage and Data Lake Analytics and an understanding of how to integrate with other Google Cloud Services.
- Using Kafka, Spark streaming and Pub/Sub to ingest real time or near real time data.
- Converting Business logic into PySpark Code for Transformation.
- In depth understanding/knowledge of Hadoop Architecture and various components such as HDFS, YARN, Name Node, Data Node, Resource Manager, Secondary Node and Map Reduce concepts.
- Hands on experience in using Apache Hadoop ecosystem components like HDFS, Hive, Sqoop, Flume, Spark, Pig and Oozie.
- Extensive experience in Data Warehousing domain (ETL domain using ODI and RDBMS (Teradata/Oracle).
- Expertise in different databases like Teradata, Oracle, IBM Netezza.
- Knowledge and hands on- experience in Python and UNIX shell Scripting.
- Hands-on in analyzing big datasets and finding patterns and insights with in unstructured and structured data.
- Have good Programming experience with Python and Scala.
- Extensive experience with SQL and database concepts.
- Involved in the Software Development Life Cycle (SDLC) various phases like Requirements, Analysis/Design, Development, Testing, Implementation and Maintenance, and Working knowledge of project methodologies (i.e. Waterfall, Agile, and Scrum).
- Ability to multi-task for different applications and Ability to take ownership and provide coordination for tasks related to implementation to test and production and Ability to deliver high-quality results under tight deadline, work In Team/Multi Diverse stake holder environment.
- Ability to adapt to evolving technology strong sense of responsibility and accomplishment.  
  </p>
</body>
</html>
